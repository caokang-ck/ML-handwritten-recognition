{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I introduce a new dataset and apply 3 algorithms(svm,kNN,NN) to see the fitness\n",
    "download dataset 2 \"USPS Handwritten Digits\"\n",
    "https://cs.nyu.edu/~roweis/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model, preprocessing\n",
    "\n",
    "\n",
    "dat = scipy.io.loadmat('C:/Users/Administrator/Desktop/introml-master/PROJECT/usps_nyu/usps_all.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'data'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1100, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dat['data']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training picture size is 16*16\n",
    "transform data into processable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 256, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABeCAYAAAAUjW5fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndJkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26a9efe1f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X=X.swapaxes(0,1)\n",
    "print(X.shape)\n",
    "\n",
    "Xsamp= X[:,:,0]\n",
    "ysamp= np.zeros((11000,))\n",
    "\n",
    "for i in range(1,10):\n",
    "    samp=X[:,:,i]\n",
    "    Xsamp=np.vstack((Xsamp,samp))\n",
    "\n",
    "for i in range(0,9):\n",
    "    for j in range(0,1100):\n",
    "        ysamp[i*1100+j]=i+1\n",
    "ysamp[9900:]=0\n",
    "\n",
    "Xdigs = Xsamp/255.0*2 - 1\n",
    "\n",
    "def plt_digit(x):\n",
    "    nrow = 16\n",
    "    ncol = 16\n",
    "    xsq = x.reshape((nrow, ncol)).swapaxes(0,1) \n",
    "    plt.imshow(xsq,   cmap='Greys_r')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "\n",
    "\n",
    "#Select random digits\n",
    "nplt  = 4\n",
    "nsamp = Xdigs.shape[0]\n",
    "Iperm = np.random.permutation(nsamp)\n",
    "\n",
    "#Plot the images using  the subplot  co   and\n",
    "for i in range (nplt):\n",
    "    ind = i\n",
    "    plt.subplot(1,nplt,i+1) \n",
    "    plt_digit(Xdigs[ind,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 1100 samples respectively for digit 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11000, 256)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xsamp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 7000\n",
    "index = np.random.permutation(num)\n",
    "ntr=num-1000\n",
    "nts=1000\n",
    "Xtr=Xsamp[index[:ntr],:]\n",
    "ytr=ysamp[index[:ntr]]\n",
    "Xts=Xsamp[index[ntr:ntr+nts],:]\n",
    "yts=ysamp[index[ntr:ntr+nts]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use model SVM, the prediction seems not satisfatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]accuracy = 0.139000\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# svc = svm.SVC(probability=False, kernel = 'rbf', C=2.8, gamma=.0073, verbose=10)\n",
    "svc = svm.SVC(probability=False, kernel = 'rbf', C=2.8, gamma=.0073, verbose=10)\n",
    "svc.fit(Xtr,ytr)\n",
    "\n",
    "yhat_ts = svc.predict(Xts)\n",
    "acc = np.mean(yhat_ts == yts)\n",
    "print('accuracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use model kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.964000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kNN = KNeighborsClassifier()\n",
    "kNN.fit(Xtr,ytr)\n",
    "\n",
    "yhat_ts1 = kNN.predict(Xts)\n",
    "acc = np.mean(yhat_ts1 == yts)\n",
    "print('accuracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following part uses Neural Network and we can see this model fits best for the USPS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "Xmean=np.mean(Xtr,axis=0)\n",
    "Xstd=np. std(Xtr,axis=0)\n",
    "Xtr_scale= (Xtr-Xmean[None, :])/Xstd[None, :]\n",
    "Xts_scale= (Xts-Xmean[None, :])/Xstd[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense,Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 68,362\n",
      "Trainable params: 68,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nin = Xtr_scale.shape[1] # dimension of input data\n",
    "nh = 256 #number of hidden units\n",
    "nout = 10 #number of outputs = 10 since there are 10 classes\n",
    "model= Sequential()\n",
    "model.add(Dense(nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "model.add(Dense(nout, activation='sigmoid', name='output'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self,logs={}):\n",
    "        # TODO:  Create two empty lists, self.loss and self.val_acc\n",
    "        self.loss = []\n",
    "        self.val_acc = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # TODO:  This is called at the end of each batch.  \n",
    "        # Add the loss in logs.get('loss') to the loss list\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # TODO:  This is called at the end of each epoch.  \n",
    "        # Add the test accuracy in logs.get('val_acc') to the val_acc list\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "        \n",
    "# Create an instance of the history callback\n",
    "history_cb = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "opt= optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=opt,\n",
    "                loss='sparse_categorical_crossentropy' ,\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/7\n",
      "6000/6000 [==============================] - 0s - loss: 1.1186 - acc: 0.7587 - val_loss: 0.3888 - val_acc: 0.9200\n",
      "Epoch 2/7\n",
      "6000/6000 [==============================] - 0s - loss: 0.2731 - acc: 0.9417 - val_loss: 0.2295 - val_acc: 0.9440\n",
      "Epoch 3/7\n",
      "6000/6000 [==============================] - 0s - loss: 0.1796 - acc: 0.9590 - val_loss: 0.1821 - val_acc: 0.9530\n",
      "Epoch 4/7\n",
      "6000/6000 [==============================] - 0s - loss: 0.1400 - acc: 0.9678 - val_loss: 0.1636 - val_acc: 0.9540\n",
      "Epoch 5/7\n",
      "6000/6000 [==============================] - 0s - loss: 0.1159 - acc: 0.9732 - val_loss: 0.1579 - val_acc: 0.9530\n",
      "Epoch 6/7\n",
      "6000/6000 [==============================] - 0s - loss: 0.0984 - acc: 0.9762 - val_loss: 0.1462 - val_acc: 0.9580\n",
      "Epoch 7/7\n",
      "6000/6000 [==============================] - 0s - loss: 0.0856 - acc: 0.9802 - val_loss: 0.1457 - val_acc: 0.9580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26aa92e7160>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "model.fit(Xtr_scale,ytr,epochs=7,batch_size=batch_size,validation_data=(Xts_scale,yts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.3616 - acc: 0.8933 - val_loss: 0.1561 - val_acc: 0.9570\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0844 - acc: 0.9755 - val_loss: 0.1360 - val_acc: 0.9620\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0411 - acc: 0.9888 - val_loss: 0.1232 - val_acc: 0.9680\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0197 - acc: 0.9967 - val_loss: 0.1188 - val_acc: 0.9710\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0099 - acc: 0.9992 - val_loss: 0.1190 - val_acc: 0.9720\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0048 - acc: 0.9998 - val_loss: 0.1211 - val_acc: 0.9720\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0034 - acc: 0.9998 - val_loss: 0.1252 - val_acc: 0.9750\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0026 - acc: 0.9998 - val_loss: 0.1283 - val_acc: 0.9700\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1316 - val_acc: 0.9710\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0014 - acc: 1.0000 - val_loss: 0.1319 - val_acc: 0.9730\n",
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 0s - loss: 1.1515 - acc: 0.7748 - val_loss: 0.4055 - val_acc: 0.9310\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.2867 - acc: 0.9377 - val_loss: 0.2339 - val_acc: 0.9440\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.1869 - acc: 0.9552 - val_loss: 0.1901 - val_acc: 0.9510\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.1469 - acc: 0.9648 - val_loss: 0.1668 - val_acc: 0.9550\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.1212 - acc: 0.9725 - val_loss: 0.1526 - val_acc: 0.9540\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.1029 - acc: 0.9763 - val_loss: 0.1465 - val_acc: 0.9570\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0886 - acc: 0.9792 - val_loss: 0.1438 - val_acc: 0.9570\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0783 - acc: 0.9817 - val_loss: 0.1394 - val_acc: 0.9590\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0689 - acc: 0.9848 - val_loss: 0.1351 - val_acc: 0.9600\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.0593 - acc: 0.9888 - val_loss: 0.1324 - val_acc: 0.9620\n",
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 0s - loss: 2.2190 - acc: 0.3123 - val_loss: 2.0386 - val_acc: 0.5740\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 0s - loss: 1.8791 - acc: 0.7183 - val_loss: 1.7385 - val_acc: 0.8060\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 0s - loss: 1.5712 - acc: 0.8288 - val_loss: 1.4153 - val_acc: 0.8420\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 0s - loss: 1.2512 - acc: 0.8575 - val_loss: 1.1108 - val_acc: 0.8610\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.9757 - acc: 0.8823 - val_loss: 0.8709 - val_acc: 0.8870\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.7717 - acc: 0.9022 - val_loss: 0.7009 - val_acc: 0.9070\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.6294 - acc: 0.9135 - val_loss: 0.5848 - val_acc: 0.9160\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.5306 - acc: 0.9210 - val_loss: 0.5029 - val_acc: 0.9280\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.4596 - acc: 0.9277 - val_loss: 0.4430 - val_acc: 0.9280\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 0s - loss: 0.4067 - acc: 0.9318 - val_loss: 0.3984 - val_acc: 0.9330\n"
     ]
    }
   ],
   "source": [
    "rates= [0.01,0.001,0.0001]\n",
    "batch_size = 100\n",
    "loss_hist = []\n",
    "val_acc_hist = []\n",
    "# TOJJO\n",
    "for lr in rates:\n",
    "    K.clear_session()\n",
    "    nin = Xtr_scale. shape[1] # dimension of input data\n",
    "    nh = 256 #number of hidden units\n",
    "    nout = 10 # number of outputs = 10 since there are 10 classes\n",
    "    model =Sequential()\n",
    "    model.add(Dense(nh, input_shape=(nin,), activation='sigmoid', name='hidden'))\n",
    "    model.add(Dense(nout, activation='sigmoid', name='output'))\n",
    "    opt= optimizers.Adam(lr=lr)\n",
    "    model.compile(optimizer=opt,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    model.fit(Xtr_scale, ytr, epochs=10, batch_size=100, validation_data=(Xts_scale,yts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
